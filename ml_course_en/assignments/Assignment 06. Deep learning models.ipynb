{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6. Basics of deep learning models\n",
    "## General Assignment. Transfer learning\n",
    "\n",
    "In this task, we will learn how to use transfer learning and fine-tuning techniques using the example of the task of recognizing hot dogs.\n",
    "\n",
    "After completing this task, you will have the opportunity to participate in the Kaggle In-Class training competition and compare your results with other course participants.\n",
    "\n",
    "Participation is completely optional and optional to complete the course. Instructions and details - at the end of the notebook!\n",
    "\n",
    "This assignment requires GPU access.\n",
    "\n",
    "It could be a GPU from NVidia on your machine, then it is recommended to install GPU-enabled PyTorch via Conda - https://pytorch.org/get-started/locally/\n",
    "\n",
    "If you don't have a GPU, you can use [Google Colab] (https://colab.research.google.com/), which provides free access to GPUs in the cloud.\n",
    "\n",
    "Google Colab setup tutorial:\n",
    "https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n",
    "(You don't need to install Keras, our notebook will install PyTorch by itself)\n",
    "\n",
    "\n",
    "\n",
    "Transfer learning and fine-tuning\n",
    "\n",
    "One of the most important techniques in training networks is to use pre-trained weights on a more general task as a starting point, and then \"train\" them on a specific one.\n",
    "\n",
    "This approach both speeds up learning and allows you to train efficient models on small datasets.\n",
    "\n",
    "In this exercise, we will train a classifier that classify hot dogs from non-hot dogs!\n",
    "\n",
    "This task requires access to the GPU, so it can be performed either on a computer with an NVidia GPU or in [Google Colab] (https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "FcXBeP1O7cnY",
    "outputId": "2b081ee6-3006-47a5-8733-ea0c317bc78e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "from socket import timeout\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "!pip3 install -q torch torchvision\n",
    "!pip3 install -q Pillow==4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the data with pictures. This will do the code in the next cell. The data will be split into two parts. On the training set, which will be stored in the ** train_kaggle ** folder, we will build our models, and on the test set ** test_kaggle ** we will predict the class to which the photo belongs (hot dog or not).\n",
    "\n",
    "### If you are on Google Colab!\n",
    "\n",
    "It can run notebooks with GPU access. They are not very fast, but they are free!\n",
    "Each notebook gets its own environment with an available disk, etc.\n",
    "\n",
    "After 90 minutes of inactivity, this environment disappears with all the data.\n",
    "Therefore, we will have to download the data every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ourBj07Arm3R",
    "outputId": "10b4ee22-fbaa-4e2f-e12d-7c3e4022f0c1"
   },
   "outputs": [],
   "source": [
    "# Download train data\n",
    "!wget \"https://storage.googleapis.com/dlcourse_ai/train.zip\"\n",
    "!unzip -q \"train.zip\"\n",
    "\n",
    "train_folder = \"train_kaggle/\"\n",
    "# Count number of files in the train folder, should be 4603\n",
    "print('Number of files in the train folder', len(os.listdir(train_folder)))\n",
    "\n",
    "# Download test data\n",
    "!wget \"https://storage.googleapis.com/dlcourse_ai/test.zip\"\n",
    "!unzip -q \"test.zip\"\n",
    "\n",
    "test_folder = \"test_kaggle/\"\n",
    "# Count number of files in the test folder, should be 1150\n",
    "print('Number of files in the test folder', len(os.listdir(test_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU-OD9O9ltP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your Dataset to load data\n",
    "\n",
    "In this task, we will implement our own Dataset class for loading data. Goal is to load data from disk and generate a tensor with the network input, label and image ID (this will make it easier to prepare a submission for kaggle on test data).\n",
    "\n",
    "Here is a link that explains well how to do this with an example: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "Your Dataset should display the number of files in a folder as the number of samples and be able to display a tuple from a sample, a score by index and file name.\n",
    "If the file name begins with the words 'frankfurter', 'chili-dog' or 'hotdog', the label is positive. Otherwise negative (zero).\n",
    "\n",
    "And don't forget to support the ability to transform the input (the `transforms` argument), we'll need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "bN2SPiJa9v5M",
    "outputId": "c5d5db7c-746c-41db-d146-30e09f7f7278"
   },
   "outputs": [],
   "source": [
    "class HotdogOrNotDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.transform = transform\n",
    "        \n",
    "        # TODO: Your code here!\n",
    "        \n",
    "    def __len__(self):\n",
    "        raise Exception(\"Not implemented!\")\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        # TODO Implement getting item by index\n",
    "        # Hint: os.path.join is helpful!\n",
    "        raise Exception(\"Not implemented!\")\n",
    "        return img, y, img_id\n",
    "\n",
    "def visualize_samples(dataset, indices, title=None, count=10):\n",
    "    # visualize random 10 samples\n",
    "    plt.figure(figsize=(count*3,3))\n",
    "    display_indices = indices[:count]\n",
    "    if title:\n",
    "        plt.suptitle(\"%s %s/%s\" % (title, len(display_indices), len(indices)))        \n",
    "    for i, index in enumerate(display_indices):    \n",
    "        x, y, _ = dataset[index]\n",
    "        plt.subplot(1,count,i+1)\n",
    "        plt.title(\"Label: %s\" % y)\n",
    "        plt.imshow(x)\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')   \n",
    "    \n",
    "orig_dataset = HotdogOrNotDataset(train_folder)\n",
    "indices = np.random.choice(np.arange(len(orig_dataset)), 7, replace=False)\n",
    "\n",
    "visualize_samples(orig_dataset, indices, \"Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "mQNsUvYm4_2V",
    "outputId": "bb771beb-38bd-40ce-a935-b27841f748ca"
   },
   "outputs": [],
   "source": [
    "# Let's make sure transforms work!\n",
    "dataset = HotdogOrNotDataset(train_folder, transform=transforms.RandomVerticalFlip(0.9))\n",
    "\n",
    "visualize_samples(dataset, indices, \"Samples with flip - a lot should be flipped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset for the training\n",
    "\n",
    "And we divide it into train and validation.\n",
    "We will train the model on train, check its quality for validation, and we will hold the Kaggle In-Class competition on photos from the test_kaggle folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAvkoRx-9FsP"
   },
   "outputs": [],
   "source": [
    "# First, lets load the dataset\n",
    "train_dataset = HotdogOrNotDataset(train_folder, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )\n",
    "test_dataset = HotdogOrNotDataset(test_folder, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRnr8CPg7Hli"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "data_size = len(dataset)\n",
    "validation_fraction = .2\n",
    "\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "# Notice that we create test data loader in a different way. We don't have the labels.\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our usual training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ek3KVQK7hJ6"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y,_) in enumerate(train_loader):\n",
    "          \n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)    \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    acc = [torch.mean((model(batch[0].to(device)).argmax(axis=1) == batch[1].to(device)).float()) for batch in loader]\n",
    "    acc = torch.mean(torch.Tensor(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained network\n",
    "\n",
    "Most often, the pre-trained network is a network trained on ImageNet data with 1M images and 1000 classes.\n",
    "\n",
    "PyTorch includes such trained networks for various architectures (https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "We will be using ResNet18.\n",
    "\n",
    "First, let's see what the already trained network gives out in our pictures. That is, let's see which of the 1000 classes the network assigns them to.\n",
    "\n",
    "Run the model on 10 random images from the dataset and output them together with the classes with the highest probability.\n",
    "There is already code in the code that generates the correspondence between the indices in the output vector and the ImageNet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CnvXSmtyLAgz",
    "outputId": "ca961672-29d0-4055-fad2-3b4c74cd500a"
   },
   "outputs": [],
   "source": [
    "# Thanks to https://discuss.pytorch.org/t/imagenet-classes/4923/2\n",
    "def load_imagenet_classes():\n",
    "    classes_json = urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json').read()\n",
    "    classes = json.loads(classes_json)\n",
    "    \n",
    "    # TODO: Process it to return dict of class index to name\n",
    "    return { int(k): v[-1] for k, v in classes.items()}\n",
    "    \n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# TODO: Run this model on 10 random images of your dataset and visualize what it predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6a-3a1ZFGEw_"
   },
   "source": [
    "# Transfer learning - train only the last layer\n",
    "\n",
    "There are several options for transfer learning, we will try the main ones.\n",
    "The first option is to replace the last layer with a new one and train only it, freezing others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jCWMUWmr7t5g",
    "outputId": "87b511e8-7ddf-4530-d218-1f03bf03cdb0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "# TODO: Freeze all the layers of this model and add a new output layer\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "parameters = None   # Fill the right thing here!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
    "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dDH4WfaB2Il"
   },
   "source": [
    "# Transfer learning - train the whole model\n",
    "\n",
    "The second option is to replace the last layer with a new one in the same way and train the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "5ss0jilyvuOh",
    "outputId": "3170f126-2b7a-405a-b63b-ccd21f94c5c2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "# TODO: Add a new output layer and train the whole model\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "parameters = None   # Fill the right thing here!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD( parameters, lr=0.001, momentum=0.9)\n",
    "\n",
    "loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meQt_vDCs9cc"
   },
   "source": [
    "# Transfer learning - different learning rates for different layers\n",
    "\n",
    "And finally, the last option that we will consider is to use different learning rates for new and old layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "evro9ksXGs9u",
    "outputId": "e4f5aca7-2e1b-4972-e061-fe9109fbeb1f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "# TODO: Add a new output layer\n",
    "# Train new layer with learning speed 0.001 and old layers with 0.0001\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = None # Hint - look into what PyTorch optimizers let you configure!\n",
    "loss_history, train_history, val_history = train_model(model_conv, train_loader, val_loader, loss, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing model metrics and errors\n",
    "\n",
    "Let's try to see where the model goes wrong - visualizing false positives and false negatives.\n",
    "\n",
    "To do this, we'll run the model through all the examples and compare it to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieEzZUglJAUB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class SubsetSampler(Sampler):\n",
    "    r\"\"\"Samples elements with given indices sequentially\n",
    "\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        indices (ndarray): indices of the samples to take\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    \n",
    "def evaluate_model(model, dataset, indices):\n",
    "    \"\"\"\n",
    "    Computes predictions and ground truth labels for the indices of the dataset\n",
    "    \n",
    "    Returns: \n",
    "    predictions: np array of booleans of model predictions\n",
    "    grount_truth: np array of boolean of actual labels of the dataset\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    \n",
    "    # TODO: Evaluate model on the list of indices and capture predictions\n",
    "    # and ground truth labels\n",
    "    # Hint: SubsetSampler above could be useful!\n",
    "    \n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    return predictions, ground_truth\n",
    "\n",
    "predictions, gt = evaluate_model(model_conv, train_dataset, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0bcioK6JBDK"
   },
   "source": [
    "And now we can render false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "WMmaPfdeKk9H",
    "outputId": "c162d02d-385c-4994-df8e-0844c2969b9f"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute indices of the false positives on the validation set.\n",
    "# Note those have to be indices of the original dataset\n",
    "false_positive_indices = None\n",
    "visualize_samples(orig_dataset, false_positive_indices, \"False positives\")\n",
    "\n",
    "# TODO: Compute indices of the false negatives on the validation set.\n",
    "# Note those have to be indices of the original dataset\n",
    "false_negatives_indices = None\n",
    "visualize_samples(orig_dataset, false_negatives_indices, \"False negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JoDeVjN4HZSV",
    "outputId": "c43261ad-524b-4a5f-ba53-fdce19c9f840"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def binary_classification_metrics(prediction, ground_truth):\n",
    "    # TODO: Implement this function!\n",
    "    # We did this already it in the assignment1\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = binary_classification_metrics(predictions, gt)\n",
    "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (f1, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_O9qiYySvuj"
   },
   "source": [
    "# What will happen at the end you already understood\n",
    "\n",
    "Train the best model based on `resnet18`, changing only the training process.\n",
    "Choose the best model by F1 score.\n",
    "\n",
    "As always, don't forget:\n",
    "- more agmentation!\n",
    "- enumeration of hyperparameters\n",
    "- various optimizers\n",
    "- what layers to tune\n",
    "- learning rate annealing\n",
    "- what era to stop at\n",
    "\n",
    "Our goal is to bring the F1 score on the validation set to a value greater than ** 0.93 **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6mhfdQ9K-N3"
   },
   "outputs": [],
   "source": [
    "# TODO: Train your best model!\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y6xExdw8JB1l",
    "outputId": "8cdf205e-14ad-4013-924a-449d06cc3eec"
   },
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set!\n",
    "predictions, ground_truth = evaluate_model(best_model, dataset, val_indices)\n",
    "precision, recall, f1 = binary_classification_metrics(predictions, ground_truth)\n",
    "print(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (precision, recall, f1))\n",
    "\n",
    "# TODO: Visualize training curve for the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the errors of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFUeNOm1VACr"
   },
   "outputs": [],
   "source": [
    "# TODO Visualize false positives and false negatives of the best model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional task\n",
    "\n",
    "Join the Kaggle In-Class Hot Dog Recognition Challenge!\n",
    "This competition is made specifically for the course and only those who take the course participate in it.\n",
    "\n",
    "In it, participants compete as trained models, uploading predictions of their models on a test sample to the site. Test dataset labeling is not available to participants.\n",
    "More details about the competition rules below.\n",
    "\n",
    "Here you can already use other basic architectures besides `resnet18`, and ensembles, and other tricks for training models.\n",
    "\n",
    "Here is the link to the competition:\n",
    "https://www.kaggle.com/c/hotdogornot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "for x,_,id_img in test_loader:\n",
    "    # TODO: Write a code to predict the tags (1 = hot dog present, 0 = no hot dog)\n",
    "    # The code should return a list of image id and predictions label\n",
    "    # image id is the name of the image file, for example '10000.jpg'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can create a csv file and then upload it to kaggle\n",
    "# Expected csv file format:\n",
    "# image_id,label\n",
    "# 10000.jpg,1\n",
    "# 10001.jpg,1\n",
    "# 10002.jpg,0\n",
    "# 10003.jpg,1\n",
    "# 10004.jpg,0\n",
    "\n",
    "with open('subm.csv', 'w') as submissionFile:\n",
    "    writer = csv.writer(submissionFile)\n",
    "    writer.writerow(['image_id', 'label'])\n",
    "    writer.writerows(zip(image_id,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And so you can download the file from Google Colab\n",
    "files.download('subm.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small introduction to Kaggle for those who have not heard of this platform before\n",
    "At its core, Kaggle is a machine learning competition platform. It appeared in 2010 and, perhaps, has become the most popular and well-known of all existing machine learning platforms. I must say that Kaggle is not only a competition, but also a community of people passionate about machine learning. In 2017 the number of registered users exceeded one million. There are also tutorials, the ability to ask questions, share code and ideas.\n",
    "\n",
    "### How is the competition going?\n",
    "Typically, participants download train data to make predictions on test data. The training set contains both the data itself and the correct labels (dependent variable values) so that the model can be trained. But the test data does not contain an answer - and our goal is to predict labels from the available data. The file with the answers for each observation from the test sample is uploaded to Kaggle and evaluated in accordance with the chosen competition metric, and the result is public and shown in the general table (also called the leaderboard) - so that there is a desire to compete and create an even stronger model. In the \"real\" competitions that take place on Kaggle, there is also a cash reward for those participants who take the first places on the leaderboard. For example, in [this] (https://www.kaggle.com/c/zillow-prize-1#description) competition, the first place winner earned about $1,000,000.\n",
    "\n",
    "The test data is divided randomly in some proportion. And while the competition is going on, the leaderboard shows the points and the rating of the participants for only one part (Public Leaderboard). But when the competition ends, the rating of the participants is compiled according to the second part of the test data (Private Leaderboard). And you can often see how people who took the first places on the public part of the test data are far from the first on the closed part of the test data. Why is this done? There are several reasons, but perhaps the most fundamental is the idea of ​​underfitting-overfitting. It is always possible that our model is tuned to a specific sample, but how will it behave on data that it has not yet seen? The division of test data into public and hidden parts is done in order to select models that have a large generalizing ability. One of the slogans of the competition participants was \"Trust your local cross-validation\" (Trust your CV!). There is a very big temptation to evaluate your model on the public part of the leaderboard, but the best strategy is to choose the model that gives the best metric for cross-validation on a training set.\n",
    "\n",
    "In our competition, the public part of the leaderboard is 30%, and the hidden part is 70%. You can make up to two attempts per day, and attempts will be scored on an F1 score. Good luck and trust your local validation! At the end of the competition, you will have the opportunity to choose 2 out of all the attempts made - the best of these two will be credited to you on the hidden part of the test data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HotdogOrNot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
